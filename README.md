# ğŸš€ LocalRAG â€” RAG Pipeline for Customer Service on local Hardware

Welcome to _LocalRAG_ â€” a focused, fast, and modular Retrieval-Augmented Generation (RAG) pipeline built around manual documentation and a curated expert dataset. This repository contains everything you need to preprocess manuals, build datasets (including synthetic LLM-generated examples), experiment with embeddings and rerankers, evaluate RAG systems, and run a local prototype chat interface.

âœ¨ Why this repo is exciting
- âš¡ Fast experiments: small, reproducible preprocessing and dataset generation pipelines.
- ğŸ”¬ Research-ready: tools for embedding model comparison, reranking, and end-to-end RAG evaluation.
- ğŸ§ª Prototype-ready: a local chatbot prototype (Docker-friendly) to demo the system and run user studies.

âš™ï¸ Repository layout
- `1_preproc/` â€” Preprocessing: HTML/manual parsing and chunking for retrieval.
- `2_datasets/` â€” Datasets: expert dataset, synthetic LLM datasets (train/test), and generation scripts.
- `3_retrieval/` â€” Retrieval experiments: embedding comparisons, lightweight fine-tuning, and advanced retrieval techniques.
- `4_RAG/` â€” RAG evaluation: system evaluation scripts and experiment notebooks using the expert dataset.
- `5_prototype/` â€” Prototype: Dockerfile, `LLM.py`, and a Streamlit demo (`streamlit.py`) to run a local chatbot interface.

ğŸ› ï¸ Project pipeline (high level)
1. ğŸ§© Convert manual HTML pages into retrieval-friendly text chunks (`1_preproc/`).
2. ğŸ§  Use those chunks to create an LLM dataset (including synthetic examples) for fine-tuning and evaluation (`2_datasets/`).
3. ğŸ” Train and compare custom embedding models, add a reranker to boost top-result quality (`3_retrieval/`).
4. ğŸ“Š Evaluate RAG systems on the expert dataset and compare different configurations (`4_RAG/`).
5. ğŸ’¬ Build a local prototype with a simple chat interface and test it on a workstation or inside Docker (`5_prototype/`).

ğŸ’¡ Notes & practical tips
- The `multilingual-e5-small` embedding model is fast and resource-efficient, but not always the best for top-tier accuracy â€” on stronger hardware consider models like `bge-m3`.
- ğŸ” Reranking improves result quality significantly; for low-resource setups, compact cross-encoder models are a pragmatic choice.
- ğŸš§ Production readiness is not complete: improvements such as robust prompt engineering, query normalization, and deployment hardening are left as next steps.

ğŸ“š Key learnings
- ğŸ§  Choose chunk sizes thoughtfully â€” dynamic chunking helps keep related passages together.
- ğŸŒ Lightweight multilingual embedding models are effective for retrieval baselines.
- ğŸ”§ Fine-tuning (including with synthetic data) can meaningfully boost retrieval performance.
- ğŸ”„ Cross-encoder rerankers help reorder candidates for better downstream LLM responses.
- âš–ï¸ LLM-based components are constrained by available local hardware; cloud or stronger GPUs enable higher performance.

â–¶ï¸ Quick start (Windows PowerShell)
1) Create and activate a virtual environment:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

2) Install requirements (prototype folder contains the app requirements):

```powershell
cd 5_prototype
pip install -r requirements.txt
```

3) Run the local prototype (Streamlit):

```powershell
cd 5_prototype
streamlit run streamlit.py
```

Enjoy exploring LocalRAG â€” build fast RAG experiments and ship prototypes quickly!