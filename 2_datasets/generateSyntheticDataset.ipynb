{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterable\n",
    "from  langchain.schema import Document\n",
    "\n",
    "#https://github.com/langchain-ai/langchain/issues/3016\n",
    "\n",
    "def load_docs_from_jsonl(file_path)->Iterable[Document]:\n",
    "    array = []\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            array.append(obj)\n",
    "    return array\n",
    "\n",
    "_chunks = load_docs_from_jsonl(\"../1_preproc/chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Du hast ein Dokument aus einem Softwarehandbuch als Kontext. Auf Basis dieses Dokuments sollst du eine oder mehrere typische Benutzerfragen und die passenden Antworten erstellen. \n",
    "Die Fragen sollten Benutzerprobleme widerspiegeln, wie z.B. \"Wo finde ich eine bestimmte Einstellung?\" oder \"Wie kann ich eine bestimmte Funktion erstellen?\". \n",
    "\n",
    "Bitte beachte die folgenden Punkte:\n",
    "1. Verwende ausschließlich Informationen aus dem gegebenen Kontext, um die Fragen und Antworten zu formulieren.\n",
    "2. Die Fragen und Antworten sollen klar, präzise und auf Deutsch verfasst sein.\n",
    "3. Stelle sicher, dass jede Frage eine direkte Antwort im Kontext hat.\n",
    "\n",
    "Gib deine Antwort im folgenden Format:\n",
    "\n",
    "Output:::\n",
    "FRAGE: (Deine Frage)\n",
    "ANTWORT: (Deine Antwort)\n",
    "\n",
    "Hier ist nun der Kontext:\n",
    "\n",
    "Kontext: {context}\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "path = \"./models/Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\"\n",
    "    \n",
    "model = Llama(\n",
    "        model_path=path,  # path to GGUF file\n",
    "        n_ctx=10000,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "        n_gpu_layers=0, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "docs = _chunks\n",
    "print(f\"Generating QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in docs:\n",
    "    # Generate QA couple\n",
    "    user_prompt = prompt.format(context=sampled_context.page_content)\n",
    "    prompt = f\"\"\"[INST] {user_prompt} [/INST]\"\"\"\n",
    "    out = model(prompt, max_tokens= 500, stop=[\"[INST]\",\"[/INST]\"], echo=True, stream=True)\n",
    "    answer = \"\"\n",
    "    for chunk in out:\n",
    "        answer += chunk[\"choices\"][0][\"text\"]\n",
    "    output_QA_couple = answer\n",
    "    print(output_QA_couple)\n",
    "    try:\n",
    "        pattern = r\"FRAGE:\\s*(.*?)\\s*ANTWORT:\\s*(.*?)(?=\\s*FRAGE:|\\Z)\"\n",
    "        # Using re.match to extract the question and answer\n",
    "        frage = \"\"\n",
    "        antwort = \"\"\n",
    "        for match in re.finditer(pattern, output_QA_couple, re.DOTALL):\n",
    "            frage = match.group(1).strip()\n",
    "            antwort = match.group(2).strip()\n",
    "            created_qa = {\n",
    "            \"context\": sampled_context.page_content,\n",
    "            \"question\": frage,\n",
    "            \"answer\": antwort,\n",
    "            \"meta\": sampled_context.metadata,\n",
    "            }\n",
    "            outputs.append(created_qa)\n",
    "            print(created_qa)\n",
    "        \n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_object = json.dumps(outputs, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./RAGDataset.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping questions for context\n",
    "In order to reduce size of our dataset, we group questions for contexts.\\\n",
    "Our *RAGGroupedDataset* only contains one question for every context chunk.\\\n",
    "We then split our data into a test and a train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the original JSON file\n",
    "with open('./RAGDataset.json', 'r', encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Dictionary to hold the new structure\n",
    "grouped_data = {}\n",
    "\n",
    "# Process each item in the original data\n",
    "for item in data:\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    if context not in grouped_data:\n",
    "        grouped_data[context] = []\n",
    "    grouped_data[context].append(question)\n",
    "\n",
    "# Convert the defaultdict to a list of dictionaries for saving\n",
    "result = [{'context': context, 'questions': questions} for context, questions in grouped_data.items()]\n",
    "\n",
    "with open('RAGGroupedDataset.json', 'w', encoding=\"utf-8\") as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320ed3e6341d48afaae596385e8453a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e7bc15cd344d13b4a9577d1b5fe7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "219194"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Load your dataset from a JSON file\n",
    "file_path = \"RAGGroupedDataset.json\"  # Replace with your JSON file path\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "for item in data:\n",
    "    processed_item = {\n",
    "            \"anchor\": item[\"questions\"][0],\n",
    "            \"positive\": item[\"context\"],\n",
    "    }\n",
    "    processed_data.append(processed_item)\n",
    "\n",
    "\n",
    "\n",
    "# Combine positive and negative samples\n",
    "final_data = processed_data\n",
    "\n",
    "# Convert data to a Dataset object\n",
    "dataset = Dataset.from_list(final_data)\n",
    "\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    " \n",
    "train_path = \"./RAG_train_dataset.json\"\n",
    "test_path = \"./RAG_test_dataset.json\"\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(train_path, orient=\"records\")\n",
    "dataset[\"test\"].to_json(test_path, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
