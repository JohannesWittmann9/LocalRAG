{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced retrieval\n",
    "In this file we:\n",
    "* Test BM25\n",
    "* Test Hybrid retrieval with BM25 and an embedding model\n",
    "* Evaluate different rerankers\n",
    "\n",
    "Results can be seen in *-hybrid-evaluation.csv* and *-rerank-evaluation.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375aa3cc8a1a4794be5b8dc7ac236260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c109049204b3484db31dc85e7b2b9fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77b5bc1c02940a59cfc1e7b9810715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018ae4625f954fb3954186a76788111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd97bc3341947d9af94f74927754c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94401db2e2d14122bcce3fbad275f187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=\"../2_datasets/RAG_test_dataset.json\", split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"../2_datasets/RAG_train_dataset.json\", split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    "\n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predicted_ranks, relevant_docs, k):\n",
    "    correct_at_k = 0\n",
    "    recall_at_k = 0\n",
    "\n",
    "    for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "        top_k_results = predicted_ranks[query_id][:k]\n",
    "        if any(doc in top_k_results for doc in relevant_doc_ids):\n",
    "            correct_at_k += 1\n",
    "        recall_at_k += len(set(relevant_doc_ids).intersection(top_k_results)) / len(relevant_doc_ids)\n",
    "\n",
    "    hit_rate = correct_at_k / len(relevant_docs)\n",
    "    recall = recall_at_k / len(relevant_docs)\n",
    "    \n",
    "    return hit_rate, recall\n",
    "\n",
    "def compute_mrr(predicted_ranks, relevant_docs, k):\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "        top_k_results = predicted_ranks[query_id][:k]\n",
    "        for rank, doc_id in enumerate(top_k_results, start=1):\n",
    "            if doc_id in relevant_doc_ids:\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # No relevant document in the top k\n",
    "\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "def writeToCSV_Embedding(model_name, results):\n",
    "    row = {\n",
    "        'model': model_name,\n",
    "        'recall@1': results['recall@1'],\n",
    "        'recall@3': results['recall@3'], \n",
    "        'recall@5': results['recall@5'],  \n",
    "        'mrr': results['mrr@10']\n",
    "    }\n",
    "\n",
    "    with open('-embedding-evaluation.csv','a', newline='') as file:\n",
    "        fields = ['model', 'recall@1', 'recall@3', 'recall@5', 'mrr']\n",
    "        writer = csv.DictWriter(file, fieldnames=fields)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus.values()] \n",
    "bm25 = BM25Okapi(tokenized_corpus)  # Initialize BM25\n",
    "\n",
    "predicted_ranks = {}\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    tokenized_query = query.split()  # Tokenize the query\n",
    "    doc_scores = bm25.get_scores(tokenized_query)  # Get BM25 scores for the query\n",
    "    ranked_doc_ids = [list(corpus.keys())[i] for i in np.argsort(doc_scores)[::-1]]  # Sort docs by score\n",
    "    predicted_ranks[q_id] = ranked_doc_ids\n",
    "\n",
    "# Calculate metrics at various cutoffs\n",
    "k_values = [1, 3, 5]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "    results[f'recall@{k}'] = recall\n",
    "    results['mrr@10'] = compute_mrr(predicted_ranks, relevant_docs, k=10)\n",
    "\n",
    "writeToCSV_Embedding(\"BM25\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Tokenize the corpus and queries\n",
    "tokenized_corpus = [doc.split() for doc in corpus.values()]  # Split corpus into tokens (words)\n",
    "bm25 = BM25Okapi(tokenized_corpus)  # Initialize BM25\n",
    "\n",
    "# Load the multilingual embedding model\n",
    "embedding_model = SentenceTransformer(\"./models/RAG-multilingual-e5-small\")\n",
    "corpus_embeddings = embedding_model.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "\n",
    "# Hybrid retrieval\n",
    "def hybrid_score(bm25_scores, embedding_scores, lambda_weight):\n",
    "    return lambda_weight * bm25_scores + (1 - lambda_weight) * embedding_scores\n",
    "\n",
    "def writeToCSV_Hybrid(model_name, results):\n",
    "    row = {\n",
    "        'model': model_name,\n",
    "        'lambda_weight': results['lambda'],\n",
    "        'recall@1': results['recall@1'],\n",
    "        'recall@3': results['recall@3'], \n",
    "        'recall@5': results['recall@5'],  \n",
    "        'mrr': results['mrr@10'],\n",
    "    }\n",
    "\n",
    "    with open('-hybrid-evaluation.csv', 'a', newline='') as file:\n",
    "        fields = ['model', 'lambda_weight','recall@1', 'recall@3', 'recall@5', 'mrr']\n",
    "        writer = csv.DictWriter(file, fieldnames=fields)\n",
    "        writer.writerow(row)\n",
    "\n",
    "lambda_values = [0.0, 0.1, 0.2, 0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9, 1.0]  # Test with different ratios of BM25 and embeddings\n",
    "for lambda_weight in lambda_values:\n",
    "    predicted_ranks = {}\n",
    "\n",
    "    # Iterate over each query\n",
    "    for q_id, query in queries.items():\n",
    "        # BM25 scores\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)  # BM25 scores\n",
    "\n",
    "        # Embedding-based scores\n",
    "        query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "        embedding_scores = cos_sim(query_embedding, corpus_embeddings).numpy().flatten()\n",
    "\n",
    "        # Combine BM25 and embedding scores using the hybrid score\n",
    "        hybrid_scores = hybrid_score(bm25_scores, embedding_scores, lambda_weight)\n",
    "        \n",
    "        # Rank documents by the hybrid score\n",
    "        ranked_doc_ids = [list(corpus.keys())[i] for i in np.argsort(hybrid_scores)[::-1]]\n",
    "        predicted_ranks[q_id] = ranked_doc_ids\n",
    "\n",
    "    # Calculate metrics\n",
    "    k_values = [1, 3, 5]\n",
    "    results = {}\n",
    "\n",
    "    for k in k_values:\n",
    "        accuracy, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "        results[f'recall@{k}'] = recall\n",
    "    \n",
    "    results['mrr@10'] = compute_mrr(predicted_ranks, relevant_docs, k=10)\n",
    "    results['lambda'] = lambda_weight\n",
    "    \n",
    "    writeToCSV_Hybrid(\"Hybrid-BM25-Embedding\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_model_1 = \"BAAI/bge-reranker-v2-m3\"  # Cross-encoder model ID\n",
    "reranker_model_2 = \"svalabs/cross-electra-ms-marco-german-uncased\"  # Cross-encoder model ID\n",
    "\n",
    "_embedding_model = \"./models/RAG-multilingual-e5-small\"\n",
    "embedding_model = \"intfloat/multilingual-e5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jWittmann\\dev\\py\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jWittmann\\dev\\py\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "\n",
    "def writeToCSV_Rerank(model_name, results):\n",
    "    row = {\n",
    "        'model': model_name,\n",
    "        'recall@1': results['recall@1'],\n",
    "        'recall@3': results['recall@3'], \n",
    "        'recall@5': results['recall@5'],  \n",
    "        'mrr': results['mrr@10'],\n",
    "    }\n",
    "\n",
    "    with open('-rerank-evaluation.csv', 'a', newline='') as file:\n",
    "        fields = ['model','recall@1', 'recall@3', 'recall@5', 'mrr']\n",
    "        writer = csv.DictWriter(file, fieldnames=fields)\n",
    "        writer.writerow(row)\n",
    "\n",
    "def evaluate(emb_model, ranker_model):\n",
    "    # Load the bi-encoder and cross-encoder models\n",
    "    bi_encoder = SentenceTransformer(emb_model)\n",
    "    cross_encoder = CrossEncoder(ranker_model)\n",
    "\n",
    "    # Encode corpus and queries using the bi-encoder\n",
    "    corpus_embeddings = bi_encoder.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "    query_embeddings = bi_encoder.encode(list(queries.values()), convert_to_tensor=True)\n",
    "\n",
    "    top_k = 10  # Number of top-k documents to retrieve\n",
    "    hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "    # List to store ranks of relevant documents\n",
    "    predicted_ranks = {}\n",
    "\n",
    "    # Re-rank using cross-encoder\n",
    "    for query_idx, query_id in enumerate(queries.keys()):\n",
    "        query_text = queries[query_id]\n",
    "\n",
    "        # Retrieve top-k hits using bi-encoder\n",
    "        top_hits = hits[query_idx]\n",
    "        doc_ids = [list(corpus.keys())[hit['corpus_id']] for hit in top_hits]\n",
    "\n",
    "        # Create query-document pairs for cross-encoder scoring\n",
    "        pairs = [(query_text, corpus[doc_id]) for doc_id in doc_ids]\n",
    "        scores = cross_encoder.predict(pairs)\n",
    "\n",
    "        # Re-rank documents based on cross-encoder scores\n",
    "        reranked_results = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "        reranked_doc_ids = [doc_id for doc_id, _ in reranked_results]\n",
    "\n",
    "        predicted_ranks[query_id] = reranked_doc_ids\n",
    "        # Logging rank changes: print(f\"For query {query_id}, Before: {doc_ids}, After: {reranked_doc_ids}\")\n",
    "    \n",
    "    \n",
    "    # Calculate metrics\n",
    "    k_values = [1, 3, 5]\n",
    "    results = {}\n",
    "\n",
    "    for k in k_values:\n",
    "        hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "        results[f'recall@{k}'] = recall\n",
    "    results['mrr@10'] = compute_mrr(predicted_ranks, relevant_docs, k=10)\n",
    "\n",
    "    writeToCSV_Rerank(ranker_model, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e375155231d42cda8c738caa286625e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b7ba2f04034441ae6562169e3dc4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f5fa8ae5e04b3e86e114c6b9363a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/381 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c9282fe514adf8f81de0525543efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/276k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e6ccdb09f46a5b2d33ab8c7d47877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/528k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f195c0f3a18e4047938cbfc29aaf3183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(_embedding_model, reranker_model_1)\n",
    "\n",
    "evaluate(_embedding_model, reranker_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jWittmann\\dev\\py\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jWittmann\\dev\\py\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "import csv\n",
    "\n",
    "def load_jsonl_with_ids(file_path):\n",
    "    documents = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            # Parse each line as a JSON object\n",
    "            doc = json.loads(line)\n",
    "            # Assign a unique ID to each document\n",
    "            doc_id = idx + 1\n",
    "            # Store the document with its ID\n",
    "            documents[doc_id] = doc[\"page_content\"]\n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "file_path = '../1_preproc/_chunks.jsonl'\n",
    "corpus = load_jsonl_with_ids(file_path)\n",
    "\n",
    "# Load your JSON dataset\n",
    "with open('../2_datasets/expert-dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a mapping for queries to multiple contexts\n",
    "queries = {}  # qid => question text\n",
    "relevant_docs = {}  # qid => set of relevant context ids\n",
    "doc_id = 0  # Counter for generating unique context ids\n",
    "\n",
    "for item in data:\n",
    "    question = item['question']\n",
    "    id = item[\"Id\"]\n",
    "    contexts = item['context']  # This is an array of contexts\n",
    "\n",
    "    if not contexts:\n",
    "        continue\n",
    "\n",
    "    queries[id] = question\n",
    "    \n",
    "\n",
    "    # Assign a unique ID to each context and add it to the corpus\n",
    "    for context in contexts:\n",
    "        content = context['page_content']  # The actual context text\n",
    "        \n",
    "        if content not in corpus.values():\n",
    "            print(f\"Error: {content} not in corpus\")\n",
    "        # Map question to relevant context ids\n",
    "        context_id = list(corpus.keys())[list(corpus.values()).index(content)]\n",
    "        if id not in relevant_docs:\n",
    "            relevant_docs[id] = []\n",
    "        relevant_docs[id].append(context_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def compute_ndcg(predicted_ranks, relevant_docs, k):\n",
    "    true_relevance = []\n",
    "    scores = []\n",
    "\n",
    "    for query_id, relevant_doc_ids in relevant_docs.items():\n",
    "        y_true = [1 if doc in relevant_doc_ids else 0 for doc in predicted_ranks[query_id][:k]]\n",
    "\n",
    "        if len(y_true) <= 1:\n",
    "            continue\n",
    "\n",
    "        true_relevance.append(y_true)\n",
    "        scores.append([1 / (i + 1) for i in range(len(y_true))])\n",
    "\n",
    "    if len(true_relevance) == 0:\n",
    "        return 0.0  # Return 0 NDCG if no valid data\n",
    "\n",
    "    ndcg_values = [ndcg_score([true], [score], k=k) for true, score in zip(true_relevance, scores)]\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "def writeToCSV_expert(model_name, results):\n",
    "    row = {\n",
    "        'model': model_name,\n",
    "        'hit-rate@1': results['hit-rate@1'],\n",
    "        'hit-rate@3': results['hit-rate@3'], \n",
    "        'hit-rate@5': results['hit-rate@5'],\n",
    "        'hit-rate@7': results['hit-rate@7'],\n",
    "        'hit-rate@10': results['hit-rate@10'],\n",
    "        'recall@1': results['recall@1'],\n",
    "        'recall@3': results['recall@3'], \n",
    "        'recall@5': results['recall@5'],\n",
    "        'recall@7': results['recall@7'],\n",
    "        'recall@10': results['recall@10'],\n",
    "        'mrr@1': results['mrr@1'],\n",
    "        'mrr@3': results['mrr@3'],\n",
    "        'mrr@5': results['mrr@5'],\n",
    "        'mrr@7': results['mrr@7'],\n",
    "        'mrr@10': results['mrr@10'],\n",
    "        'ndcg@1': 0.0,\n",
    "        'ndcg@3': results['ndcg@3'],\n",
    "        'ndcg@5': results['ndcg@5'],\n",
    "        'ndcg@7': results['ndcg@7'],\n",
    "        'ndcg@10': results['ndcg@10']\n",
    "    }\n",
    "\n",
    "    with open('-embedding-evaluation-expert.csv','a', newline='') as file:\n",
    "        fields = ['model']\n",
    "        fields.extend([f\"hit-rate@{i}\" for i in [1,3,5,7,10]])\n",
    "        fields.extend([f\"recall@{i}\" for i in [1,3,5,7,10]])\n",
    "        fields.extend([f\"mrr@{i}\" for i in [1,3,5,7,10]])\n",
    "        fields.extend([f\"ndcg@{i}\" for i in [1,3,5,7,10]])\n",
    "        writer = csv.DictWriter(file, fieldnames=fields)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus.values()] \n",
    "bm25 = BM25Okapi(tokenized_corpus)  # Initialize BM25\n",
    "\n",
    "predicted_ranks = {}\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    tokenized_query = query.split()  # Tokenize the query\n",
    "    doc_scores = bm25.get_scores(tokenized_query)  # Get BM25 scores for the query\n",
    "    ranked_doc_ids = [list(corpus.keys())[i] for i in np.argsort(doc_scores)[::-1]]  # Sort docs by score\n",
    "    predicted_ranks[q_id] = ranked_doc_ids\n",
    "\n",
    "# Calculate metrics at various cutoffs\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "    results[f'hit-rate@{k}'] = hit_rate\n",
    "    results[f'recall@{k}'] = recall\n",
    "    results[f'ndcg@{k}'] = compute_ndcg(predicted_ranks, relevant_docs, k=k)\n",
    "    results[f'mrr@{k}'] = compute_mrr(predicted_ranks, relevant_docs, k=k)\n",
    "\n",
    "writeToCSV_expert(\"BM25\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the bi-encoder and cross-encoder models\n",
    "bi_encoder = SentenceTransformer(embedding_model)\n",
    "\n",
    "# Encode corpus and queries using the bi-encoder\n",
    "corpus_embeddings = bi_encoder.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "query_embeddings = bi_encoder.encode(list(queries.values()), convert_to_tensor=True)\n",
    "\n",
    "top_k = 10  # Number of top-k documents to retrieve\n",
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "# List to store ranks of relevant documents\n",
    "predicted_ranks = {}\n",
    "\n",
    "# Re-rank using cross-encoder\n",
    "for query_idx, query_id in enumerate(queries.keys()):\n",
    "    query_text = queries[query_id]\n",
    "\n",
    "    # Retrieve top-k hits using bi-encoder\n",
    "    top_hits = hits[query_idx]\n",
    "    doc_ids = [list(corpus.keys())[hit['corpus_id']] for hit in top_hits]\n",
    "\n",
    "    predicted_ranks[query_id] = doc_ids\n",
    "    \n",
    "    \n",
    "# Calculate metrics\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "    results[f'hit-rate@{k}'] = hit_rate\n",
    "    results[f'recall@{k}'] = recall\n",
    "    results[f'ndcg@{k}'] = compute_ndcg(predicted_ranks, relevant_docs, k=k)\n",
    "    results[f'mrr@{k}'] = compute_mrr(predicted_ranks, relevant_docs, k=k)\n",
    "\n",
    "writeToCSV_expert(\"Base-embedding\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the bi-encoder and cross-encoder models\n",
    "bi_encoder = SentenceTransformer(_embedding_model)\n",
    "\n",
    "# Encode corpus and queries using the bi-encoder\n",
    "corpus_embeddings = bi_encoder.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "query_embeddings = bi_encoder.encode(list(queries.values()), convert_to_tensor=True)\n",
    "\n",
    "top_k = 10  # Number of top-k documents to retrieve\n",
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "# List to store ranks of relevant documents\n",
    "predicted_ranks = {}\n",
    "\n",
    "# Re-rank using cross-encoder\n",
    "for query_idx, query_id in enumerate(queries.keys()):\n",
    "    query_text = queries[query_id]\n",
    "\n",
    "    # Retrieve top-k hits using bi-encoder\n",
    "    top_hits = hits[query_idx]\n",
    "    doc_ids = [list(corpus.keys())[hit['corpus_id']] for hit in top_hits]\n",
    "\n",
    "    predicted_ranks[query_id] = doc_ids\n",
    "    \n",
    "    \n",
    "# Calculate metrics\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "    results[f'hit-rate@{k}'] = hit_rate\n",
    "    results[f'recall@{k}'] = recall\n",
    "    results[f'ndcg@{k}'] = compute_ndcg(predicted_ranks, relevant_docs, k=k)\n",
    "    results[f'mrr@{k}'] = compute_mrr(predicted_ranks, relevant_docs, k=k)\n",
    "\n",
    "writeToCSV_expert(\"Fine-tune\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "\n",
    "# Load the bi-encoder and cross-encoder models\n",
    "bi_encoder = SentenceTransformer(_embedding_model)\n",
    "cross_encoder = CrossEncoder(reranker_model_2)\n",
    "\n",
    "# Encode corpus and queries using the bi-encoder\n",
    "corpus_embeddings = bi_encoder.encode(list(corpus.values()), convert_to_tensor=True)\n",
    "query_embeddings = bi_encoder.encode(list(queries.values()), convert_to_tensor=True)\n",
    "\n",
    "top_k = 10  # Number of top-k documents to retrieve\n",
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "# List to store ranks of relevant documents\n",
    "predicted_ranks = {}\n",
    "\n",
    "# Re-rank using cross-encoder\n",
    "for query_idx, query_id in enumerate(queries.keys()):\n",
    "    query_text = queries[query_id]\n",
    "\n",
    "    # Retrieve top-k hits using bi-encoder\n",
    "    top_hits = hits[query_idx]\n",
    "    doc_ids = [list(corpus.keys())[hit['corpus_id']] for hit in top_hits]\n",
    "\n",
    "    # Create query-document pairs for cross-encoder scoring\n",
    "    pairs = [(query_text, corpus[doc_id]) for doc_id in doc_ids]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Re-rank documents based on cross-encoder scores\n",
    "    reranked_results = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked_doc_ids = [doc_id for doc_id, _ in reranked_results]\n",
    "\n",
    "    predicted_ranks[query_id] = reranked_doc_ids\n",
    "    # Logging rank changes: print(f\"For query {query_id}, Before: {doc_ids}, After: {reranked_doc_ids}\")\n",
    "    \n",
    "    \n",
    "# Calculate metrics\n",
    "k_values = [1, 3, 5, 7, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    hit_rate, recall = compute_metrics(predicted_ranks, relevant_docs, k)\n",
    "    results[f'hit-rate@{k}'] = hit_rate\n",
    "    results[f'recall@{k}'] = recall\n",
    "    results[f'ndcg@{k}'] = compute_ndcg(predicted_ranks, relevant_docs, k=k)\n",
    "    results[f'mrr@{k}'] = compute_mrr(predicted_ranks, relevant_docs, k=k)\n",
    "\n",
    "writeToCSV_expert(\"Rerank\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
