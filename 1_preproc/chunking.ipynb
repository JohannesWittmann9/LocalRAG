{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "#### Data insights\n",
    "Chunking is crucial for a RAG system. Depending on the data source, there is no gold standard for chosing methods and sizes.\n",
    "Therefore, we first try to gain insights about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotated context has a total of 18027 words.\n",
      "Thats an average of rounded 237 words in contexts.\n",
      "The longest context piece is 848 words long.\n",
      "The shortest context piece is 30 words short.\n",
      "On average, a question can be answered with 1.4339622641509433 context pieces.\n",
      "The max number of context pieces to answer a question is 6 .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"../2_datasets/expert-dataset.json\"\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "words_sum = 0 # summary of words in contexts\n",
    "context_pieces = 0 # number of context pieces in expert data\n",
    "\n",
    "context_max_length = 0 # minimal length of context pieces\n",
    "context_min_length = 1000 # mixmum length of context pieces\n",
    "\n",
    "max_num_context_for_question = 0 # maximum number of context pieces needed for a question\n",
    "\n",
    "for js in data:\n",
    "    contextArray = js[\"context\"]\n",
    "    if len(contextArray) > max_num_context_for_question:\n",
    "        max_num_context_for_question = len(contextArray)\n",
    "    for ctx in contextArray:\n",
    "        length = len(ctx[\"page_content\"].split())\n",
    "        words_sum += length\n",
    "        context_pieces += 1\n",
    "        if context_max_length < length:\n",
    "            context_max_length = length\n",
    "        if context_min_length > length:\n",
    "            context_min_length = length\n",
    "\n",
    "avg_ctx_pieces = context_pieces / len(data)\n",
    "avg_ctx_length = round(words_sum/context_pieces)\n",
    "\n",
    "print(\"Anotated context has a total of\", words_sum, \"words.\")\n",
    "print(\"Thats an average of rounded\",avg_ctx_length , \"words in contexts.\")\n",
    "print(\"The longest context piece is\", context_max_length, \"words long.\")\n",
    "print(\"The shortest context piece is\", context_min_length, \"words short.\")\n",
    "print(\"On average, a question can be answered with\", avg_ctx_pieces,\"context pieces.\")\n",
    "print(\"The max number of context pieces to answer a question is\", max_num_context_for_question,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML to Markdown\n",
    "We load our HTML data, remove irrelevant tags and replace e.g. images and icons. Utilizing the langchain **HTML2TextTransformer** we then process our HTML to Markdown. Results can be found in the 'markdown' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "def processHTMLContents(content):\n",
    "    for img in content.find_all('img'):\n",
    "        path = img['src']\n",
    "\n",
    "        # Replacing images and icons with custom tags linked to source\n",
    "        icons_path = \"\"\n",
    "        images_path = \"\"\n",
    "        if path.startswith(icons_path):        \n",
    "            img.replace_with('![Icon]('+path+')')\n",
    "        \n",
    "        if path.startswith(images_path):\n",
    "            img.replace_with('![Image]('+path+')')\n",
    "\n",
    "    # Replace X icons in a checkbox table\n",
    "    for tag in content.find_all('i', class_='fal fa-check fa-2x'):\n",
    "        tag.replace_with('X')\n",
    "        \n",
    "    # Remove noise like header and button elements\n",
    "    for header in content.find_all('header'):\n",
    "        header.decompose()\n",
    "\n",
    "    for btn in content.find_all('button'):\n",
    "        btn.decompose()\n",
    "\n",
    "# Returns Markdown elements\n",
    "def parse(file_path):\n",
    "    directory = \"./html\"\n",
    "    file_path_ = os.path.join(directory, file_path)\n",
    "    with open(file_path_, encoding=\"utf8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    content = soup.find(class_=\"TopicViewer_container\") # Content of HTML\n",
    "\n",
    "    processHTMLContents(content)\n",
    "\n",
    "    content.text.replace('\\xa0', '')\n",
    "    \n",
    "    # Add document source as metadata\n",
    "    doc = Document(page_content=str(soup), metadata={\"source\": file_path})\n",
    "\n",
    "    # Transform to markdown\n",
    "    html2text = Html2TextTransformer()\n",
    "    docs_transformed = html2text.transform_documents([doc])\n",
    "\n",
    "    return docs_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./html\"\n",
    "files = os.listdir(directory)\n",
    "output_directory = \"./markdown/\"\n",
    "\n",
    "# Filter HTML files\n",
    "html_files = [file for file in files if file.endswith('.html')]\n",
    "\n",
    "document_chunks = []\n",
    "\n",
    "for file in html_files:\n",
    "    chunks = parse(file)\n",
    "    document_chunks.append(chunks)\n",
    "    text_file_name = file.replace('.html', '.md')\n",
    "    output_path = output_directory + text_file_name\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting markdown files by headers\n",
    "With the langchain **MarkdownHeaderTextSplitter** we can split our documents without loosing relevant context in chunks.\n",
    "We append header text information to our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_dir = './markdown/'\n",
    "md_files = os.listdir(markdown_dir)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for md in md_files:\n",
    "    md_path = markdown_dir + md\n",
    "    with open(md_path, encoding=\"utf8\") as f:\n",
    "        md_txt = f.read()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\")\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "    for chunk in markdown_splitter.split_text(md_txt):\n",
    "       meta = chunk.metadata\n",
    "       meta[\"source-document\"] = md\n",
    "       chunks.append(Document(page_content=chunk.page_content, metadata=meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting HTML files into 521 markdown chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Splitting HTML files into {len(chunks)} markdown chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 4\n",
      "Max: 1874\n",
      "Average: 222.43186180422265\n",
      "Median: 134\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "max = 0\n",
    "min = 1000\n",
    "avg = 0\n",
    "c_list = []\n",
    "\n",
    "for item in chunks:\n",
    "    c = item.page_content\n",
    "    chunks = c.split()\n",
    "    num_chunks = len(chunks)\n",
    "    if num_chunks > max:\n",
    "        max = num_chunks\n",
    "    if num_chunks < min:\n",
    "        min_c = chunks\n",
    "        min = num_chunks\n",
    "    avg += num_chunks\n",
    "    c_list.append(num_chunks)\n",
    "    \n",
    "print(\"Min:\", min)\n",
    "print(\"Max:\", max)\n",
    "print(\"Average:\", avg/len(chunks))\n",
    "print(\"Median:\", statistics.median(c_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a minimum word length of *4* might not be able to contain relevant information we see, it is a good decision to not split our data with a static chunk size.\n",
    "However, we see medium and median are pretty close in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain.schema import Document\n",
    "import json\n",
    "from typing import Iterable\n",
    "\n",
    "#https://github.com/langchain-ai/langchain/issues/3016\n",
    "\n",
    "def save_docs_to_jsonl(array:Iterable[Document], file_path:str)->None:\n",
    "    with open(file_path, 'w') as jsonl_file:\n",
    "        for doc in array:\n",
    "            jsonl_file.write(doc.json() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our processed chunks to *chunks.jsonl*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_path = \"./chunks.jsonl\"\n",
    "save_docs_to_jsonl(chunks, chunks_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
